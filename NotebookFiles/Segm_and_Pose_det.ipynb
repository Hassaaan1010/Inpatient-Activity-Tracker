{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from stopwatch import Stopwatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentationModel = YOLO(\"YOLO_models/yolov8n-seg.pt\")\n",
    "# n s m l x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation color map\n",
    "fixed_color_map = [\n",
    "    [255, 0, 0],  # Red\n",
    "    [0, 255, 0],  # Green\n",
    "    [0, 0, 255],  # Blue\n",
    "    [255, 255, 0],  # Yellow\n",
    "    [255, 0, 255],  # Magenta\n",
    "    [0, 255, 255],  # Cyan\n",
    "    [128, 0, 0],  # Maroon\n",
    "    [128, 128, 0],  # Olive\n",
    "    [0, 128, 0],  # Dark Green\n",
    "    [128, 0, 128],  # Purple\n",
    "    [0, 128, 128],  # Teal\n",
    "    [0, 0, 128],  # Navy\n",
    "    [192, 192, 192],  # Silver\n",
    "    [128, 128, 128],  # Gray\n",
    "    [255, 165, 0],  # Orange\n",
    "    [255, 192, 203],  # Pink\n",
    "    [75, 0, 130],  # Indigo\n",
    "    [245, 222, 179],  # Wheat\n",
    "    [255, 228, 196],  # Bisque\n",
    "    [34, 139, 34],  # Forest Green\n",
    "    [255, 215, 0],  # Gold\n",
    "    [173, 216, 230],  # Light Blue\n",
    "    [0, 255, 127],  # Spring Green\n",
    "    [70, 130, 180],  # Steel Blue\n",
    "    [255, 69, 0],  # Red-Orange\n",
    "    [124, 252, 0],  # Lawn Green\n",
    "    [0, 206, 209],  # Dark Turquoise\n",
    "    [147, 112, 219],  # Medium Purple\n",
    "    [199, 21, 133],  # Medium Violet-Red\n",
    "    [255, 99, 71],  # Tomato\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close camera and output window for termination\n",
    "cap = object()\n",
    "\n",
    "\n",
    "def closeAll():\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Calculate angle for 3 landmarks.\n",
    "def calculateAngle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(\n",
    "        a[1] - b[1], a[0] - b[0]\n",
    "    )\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "\n",
    "    if angle > 180.0:\n",
    "        angle = 360.0 - angle\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentFrame(frame):\n",
    "\n",
    "    # Perform segmentation\n",
    "    results = segmentationModel.predict(frame, task=\"segment\")\n",
    "\n",
    "    # Extract the segmentation masks\n",
    "    masks = results[0].masks.data\n",
    "\n",
    "    # Create a color map for the masks\n",
    "    # color_map = np.random.randint(0, 255, (len(masks), 3), dtype=np.uint8)\n",
    "\n",
    "    if len(fixed_color_map) < len(masks):\n",
    "        raise ValueError(\n",
    "            \"The fixed color map does not have enough colors for all masks.\"\n",
    "        )\n",
    "\n",
    "    # color\n",
    "    for i, mask in enumerate(masks):\n",
    "        color = fixed_color_map[i]\n",
    "        binary_mask = mask.cpu().numpy() > 0.5  # Convert to binary mask\n",
    "        binary_mask = binary_mask.astype(np.uint8) * 255  # Convert to uint8\n",
    "\n",
    "        # Find contours and draw them on the frame\n",
    "        contours, _ = cv2.findContours(\n",
    "            binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        cv2.drawContours(frame, contours, -1, color, thickness=cv2.FILLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose detection by mediapipe\n",
    "def poseDetection(frame, pose):\n",
    "    # recolor image to rgb from cv2 default (bgr)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "\n",
    "    # make detection and store in result\n",
    "    # writeable flag is unset before processing to improve performance and avoid unintended write ops.\n",
    "    # frame is read and processed and the data is written to results.\n",
    "    results = pose.process(frame)\n",
    "\n",
    "    # convert back to cv2 default bgr\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# marking landmarks, pose and joining lines\n",
    "def poseMarking(frame, results):\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "    shoulderStats = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    hipStats = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "    ankleStats = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value]\n",
    "\n",
    "    shoulder = [\n",
    "        shoulderStats.x,\n",
    "        shoulderStats.y,\n",
    "    ]\n",
    "    hip = [\n",
    "        hipStats.x,\n",
    "        hipStats.y,\n",
    "    ]\n",
    "    ankle = [\n",
    "        ankleStats.x,\n",
    "        ankleStats.y,\n",
    "    ]\n",
    "\n",
    "    angle = int(calculateAngle(shoulder, hip, ankle))\n",
    "\n",
    "    # dictionary for label : coordinates\n",
    "    dct = {\n",
    "        \"shoulder\": tuple(np.multiply(shoulder, [640, 480]).astype(int)),\n",
    "        \"hip\": tuple(\n",
    "            np.multiply(hip, [640, 480]).astype(int)\n",
    "            + np.array([0, -10])  # -10 to avoid overlap with angle\n",
    "        ),\n",
    "        \"ankle\": tuple(np.multiply(ankle, [640, 480]).astype(int)),\n",
    "        str(angle): tuple(\n",
    "            np.multiply(hip, [640, 480]).astype(int)\n",
    "            + np.array([0, 10])  # +10 to avoid overlap with hip\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # add labels and angle measure\n",
    "    for part in dct:\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            part,\n",
    "            tuple(dct[part]),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    # draw landmarks and connecting lines\n",
    "    mp_drawing.draw_landmarks(\n",
    "        frame,  # output\n",
    "        results.pose_landmarks,  # passing landmarks\n",
    "        mp_pose.POSE_CONNECTIONS,  # passing landmark connections\n",
    "        mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "        mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    return angle\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720689276.711633  162094 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1720689276.734878  162705 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: Mesa Intel(R) HD Graphics 4400 (HSW GT2)\n",
      "W0000 00:00:1720689276.945417  162701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1720689276.995452  162701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 remote, 301.4ms\n",
      "Speed: 27.4ms preprocess, 301.4ms inference, 18.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 241.8ms\n",
      "Speed: 3.4ms preprocess, 241.8ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 239.0ms\n",
      "Speed: 3.8ms preprocess, 239.0ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 275.1ms\n",
      "Speed: 4.2ms preprocess, 275.1ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 237.7ms\n",
      "Speed: 3.0ms preprocess, 237.7ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 233.0ms\n",
      "Speed: 2.2ms preprocess, 233.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in segmentation : 'NoneType' object has no attribute 'data'\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 bed, 242.1ms\n",
      "Speed: 3.1ms preprocess, 242.1ms inference, 9.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 237.2ms\n",
      "Speed: 4.4ms preprocess, 237.2ms inference, 7.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 236.9ms\n",
      "Speed: 2.7ms preprocess, 236.9ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 1 remote, 240.4ms\n",
      "Speed: 3.6ms preprocess, 240.4ms inference, 19.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 238.7ms\n",
      "Speed: 3.1ms preprocess, 238.7ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 237.0ms\n",
      "Speed: 2.9ms preprocess, 237.0ms inference, 7.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 person, 1 suitcase, 1 bed, 236.2ms\n",
      "Speed: 3.0ms preprocess, 236.2ms inference, 17.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bed, 1 remote, 242.7ms\n",
      "Speed: 3.5ms preprocess, 242.7ms inference, 13.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bed, 244.9ms\n",
      "Speed: 3.3ms preprocess, 244.9ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 suitcase, 1 bed, 1 remote, 241.9ms\n",
      "Speed: 3.1ms preprocess, 241.9ms inference, 12.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 248.3ms\n",
      "Speed: 3.3ms preprocess, 248.3ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 243.4ms\n",
      "Speed: 2.6ms preprocess, 243.4ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 247.9ms\n",
      "Speed: 3.3ms preprocess, 247.9ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 241.6ms\n",
      "Speed: 3.9ms preprocess, 241.6ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 247.2ms\n",
      "Speed: 2.4ms preprocess, 247.2ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 249.8ms\n",
      "Speed: 2.6ms preprocess, 249.8ms inference, 7.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 person, 1 suitcase, 1 bed, 241.8ms\n",
      "Speed: 2.3ms preprocess, 241.8ms inference, 12.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 248.9ms\n",
      "Speed: 3.4ms preprocess, 248.9ms inference, 7.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 242.3ms\n",
      "Speed: 2.3ms preprocess, 242.3ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 236.6ms\n",
      "Speed: 2.8ms preprocess, 236.6ms inference, 7.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 250.7ms\n",
      "Speed: 5.9ms preprocess, 250.7ms inference, 7.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 238.4ms\n",
      "Speed: 3.0ms preprocess, 238.4ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 252.9ms\n",
      "Speed: 2.7ms preprocess, 252.9ms inference, 8.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 361.1ms\n",
      "Speed: 3.3ms preprocess, 361.1ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 235.7ms\n",
      "Speed: 3.6ms preprocess, 235.7ms inference, 8.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 person, 1 suitcase, 1 bed, 248.7ms\n",
      "Speed: 2.9ms preprocess, 248.7ms inference, 14.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 person, 251.5ms\n",
      "Speed: 2.8ms preprocess, 251.5ms inference, 5.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bed, 252.2ms\n",
      "Speed: 3.0ms preprocess, 252.2ms inference, 7.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bed, 240.1ms\n",
      "Speed: 2.5ms preprocess, 240.1ms inference, 7.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bed, 268.6ms\n",
      "Speed: 3.8ms preprocess, 268.6ms inference, 9.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bed, 262.8ms\n",
      "Speed: 3.7ms preprocess, 262.8ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 suitcase, 1 bed, 250.3ms\n",
      "Speed: 2.7ms preprocess, 250.3ms inference, 13.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 260.3ms\n",
      "Speed: 2.5ms preprocess, 260.3ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 247.8ms\n",
      "Speed: 2.3ms preprocess, 247.8ms inference, 7.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 257.4ms\n",
      "Speed: 4.2ms preprocess, 257.4ms inference, 9.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 251.0ms\n",
      "Speed: 2.2ms preprocess, 251.0ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 253.9ms\n",
      "Speed: 2.9ms preprocess, 253.9ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 259.3ms\n",
      "Speed: 3.4ms preprocess, 259.3ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 254.2ms\n",
      "Speed: 2.8ms preprocess, 254.2ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 261.6ms\n",
      "Speed: 3.0ms preprocess, 261.6ms inference, 7.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 suitcase, 1 bed, 253.5ms\n",
      "Speed: 4.2ms preprocess, 253.5ms inference, 7.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error in landmarks output\n",
      "\n",
      "0: 480x640 1 person, 1 bed, 251.3ms\n",
      "Speed: 2.5ms preprocess, 251.3ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bed, 1 remote, 254.4ms\n",
      "Speed: 3.5ms preprocess, 254.4ms inference, 14.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 keyboard, 277.4ms\n",
      "Speed: 2.4ms preprocess, 277.4ms inference, 6.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 251.8ms\n",
      "Speed: 2.6ms preprocess, 251.8ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "running = True\n",
    "landmarksAvailable = (\n",
    "    True  # to flag landmarks not available for marking in case person not in frame\n",
    ")\n",
    "\n",
    "# CAMERA MEDIA PIPING\n",
    "lying = Stopwatch(2)\n",
    "sitting = Stopwatch(2)\n",
    "state = None  # can be \"standing\", \"sitting\", \"lying\"\n",
    "\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while running:\n",
    "\n",
    "        # capture single frame\n",
    "        # ~50ms per run. Can be removed if camera has FPS customization. 10 FPS\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # 1 Find Body Landmarks\n",
    "        try:\n",
    "            results = poseDetection(frame, pose)\n",
    "        except Exception as e:\n",
    "            # closeAll()\n",
    "            print(\"Error with pose detection\", e)\n",
    "            landmarksAvailable = False\n",
    "            pass  # to prevent crash when no person pose is detected.\n",
    "\n",
    "        # 2 Segment and Color Objects\n",
    "        try:\n",
    "            segmentFrame(frame)\n",
    "        except Exception as e:\n",
    "            # closeAll()\n",
    "            print(\"Error in segmentation :\", e)\n",
    "\n",
    "        # 3 Apply Landmarks and Angles\n",
    "        if landmarksAvailable:\n",
    "            try:\n",
    "                # angle is returned after marking\n",
    "                angle = poseMarking(frame, results)\n",
    "            except:\n",
    "                print(\"Error in landmarks output\")\n",
    "                continue\n",
    "\n",
    "        # change to lying\n",
    "        if state != \"lying\" and angle > 150:\n",
    "            lying.start()\n",
    "            sitting.stop()\n",
    "            state = \"lying\"\n",
    "\n",
    "        # change to sitting\n",
    "        if state == \"lying\" and angle < 150:\n",
    "            sitting.start()\n",
    "            lying.stop()\n",
    "            state = \"sitting\"\n",
    "\n",
    "        cv2.rectangle(frame, (0, 0), (225, 73), (245, 117, 16), -1)\n",
    "\n",
    "        displayDict = {\n",
    "            \"ANGLE\": (15, 12),\n",
    "            str(angle): (15, 40),\n",
    "            \"STATE\": (85, 12),\n",
    "            state: (85, 40),\n",
    "            \"Lying\": (135, 12),\n",
    "            str(int(lying.duration)): (135, 40),\n",
    "            \"Sitting\": (185, 12),\n",
    "            str(int(sitting.duration)): (185, 40),\n",
    "        }\n",
    "\n",
    "        for text in displayDict:\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                text,\n",
    "                displayDict[text],\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 0, 0),\n",
    "                1,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Video\", frame)\n",
    "\n",
    "        # wait for \"q\" click to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            running = False\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
